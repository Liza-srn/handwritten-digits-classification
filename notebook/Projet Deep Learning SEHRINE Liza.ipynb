{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7570f74",
   "metadata": {},
   "source": [
    "# PROJET DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63126e6d",
   "metadata": {},
   "source": [
    "## Alpha Digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea2e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clés du dictionnaire : dict_keys(['__header__', '__version__', '__globals__', 'dat', 'numclass', 'classlabels', 'classcounts'])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "# Spécifiez le chemin du fichier .mat\n",
    "digits = \"binaryalphadigs.mat\"\n",
    "\n",
    "# Chargez les données MATLAB\n",
    "mat_data_digits = scipy.io.loadmat(digits)\n",
    "\n",
    "# Récupérez les clés du dictionnaire\n",
    "keys = mat_data_digits.keys()\n",
    "\n",
    "dat_array = mat_data_digits['dat']\n",
    "\n",
    "# Affichez les clés\n",
    "print(\"Clés du dictionnaire :\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ab147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_alpha_digit(filename: str, indices=None):\n",
    "    mat = sp.io.loadmat(filename, simplify_cells=True)\n",
    "    bad = mat_data_digits[\"dat\"][indices, :]\n",
    "    images = np.zeros((bad.size, bad[0, 0].size))\n",
    "    im = 0  # image index\n",
    "    for i in range(bad.shape[0]):\n",
    "        for j in range(bad.shape[1]):\n",
    "            images[im, :] = bad[i, j].flatten()\n",
    "            im += 1\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc971bf",
   "metadata": {},
   "source": [
    "# RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98705a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def logit(z):\n",
    "    return np.log(z / (1 - z))\n",
    "\n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, p, q):\n",
    "        self.a = np.zeros(p)\n",
    "        self.b = np.zeros(q)\n",
    "        self.W = np.random.normal(size=(p, q)) * np.sqrt(0.01)\n",
    "\n",
    "    def entree_sortie_rbm(self, v):\n",
    "        return sigmoid(self.b + np.dot(v, self.W))\n",
    "\n",
    "    def sortie_entree_rbm(self, h):\n",
    "        return sigmoid(self.a + np.dot(h, self.W.T))\n",
    "\n",
    "    def train_rbm(\n",
    "        self, X, epochs, learning_rate, batch_size, verbose=False\n",
    "    ):\n",
    "        #keep_track = 0\n",
    "        weights = []\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            data_copy = X.copy()\n",
    "            np.random.shuffle(data_copy)\n",
    "\n",
    "            for batch in range(0, X.shape[0], batch_size):\n",
    "                data_batch = data_copy[\n",
    "                    batch: min(batch + batch_size, X.shape[0]), :\n",
    "                ]\n",
    "\n",
    "                v0 = data_batch\n",
    "                p_h_0 = self.entree_sortie_rbm(v0)\n",
    "                h_0 = np.random.binomial(1, p_h_0)\n",
    "                p_v_1 = self.sortie_entree_rbm(h_0)\n",
    "                v1 = np.random.binomial(1, p_v_1)\n",
    "                p_h_1 = self.entree_sortie_rbm(v1)\n",
    "\n",
    "                grad_w = np.dot(v0.T, p_h_0) - np.dot(v1.T, p_h_1)\n",
    "                grad_a = np.sum(v0 - v1, axis=0)\n",
    "                grad_b = np.sum(p_h_0 - p_h_1, axis=0)\n",
    "\n",
    "                self.W += learning_rate / batch_size * grad_w\n",
    "                self.a += learning_rate / batch_size * grad_a\n",
    "                self.b += learning_rate / batch_size * grad_b\n",
    "                \n",
    "                weights.append(np.mean(self.W))\n",
    "\n",
    "            h_epoch = self.entree_sortie_rbm(X)\n",
    "            data_rec = self.sortie_entree_rbm(h_epoch)\n",
    "            mse = np.sum((data_rec - X) ** 2) / X.size\n",
    "            losses.append(mse)\n",
    "            \n",
    "            # Décommenter les lignes suivantes pour Afficher la fonction de perte\n",
    "            \n",
    "            #if keep_track < early_stopping and round(mse, 3) == round(previous_mse, 3):\n",
    "             #   keep_track += 1\n",
    "            #elif keep_track == early_stopping:\n",
    "                #return self\n",
    "            #if verbose:\n",
    "             #   print(f\"Epoch {epoch+1}/{epochs} - Error: {mse:.3f}\")\n",
    "\n",
    "        #plt.plot(losses)\n",
    "        #plt.xlabel('epochs')\n",
    "        #plt.ylabel('loss')\n",
    "        #plt.title('Evolution of the loss through ' + str(epochs) + ' epochs')\n",
    "        #plt.show()\n",
    "        #print(\"Final loss :\", losses[-1])\n",
    "\n",
    "        #plt.xlabel('epochs')\n",
    "        #plt.ylabel('mean elements of weight W')\n",
    "        #plt.plot(weights)\n",
    "        #plt.show()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate_image_rbm(self, x_im, y_im, nb_images, nb_gibbs, plot=False):\n",
    "        p = self.W.shape[0]\n",
    "        images = np.zeros((nb_images, p))\n",
    "        for data in range(nb_images):\n",
    "            v = np.random.binomial(1, 0.5 * np.ones(p))\n",
    "            for iter_gibbs in range(nb_gibbs):\n",
    "                h = np.random.binomial(1, self.entree_sortie_rbm(v))\n",
    "                v = np.random.binomial(1, self.sortie_entree_rbm(h))\n",
    "\n",
    "            images[data, :] = v\n",
    "\n",
    "        if plot:\n",
    "            # Reshape and Plot the generated images\n",
    "            fig, axes = plt.subplots(1, nb_images, figsize=(10, 2))\n",
    "            images = images.reshape((nb_images, x_im, y_im))\n",
    "            for i in range(nb_images):\n",
    "                axes[i].imshow(images[i], cmap=\"gray\")\n",
    "                axes[i].axis(\"off\")\n",
    "            plt.show()\n",
    "        return images\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d463ba1",
   "metadata": {},
   "source": [
    "## Test RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa33fba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m nb_gibbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      9\u001b[0m rbm \u001b[38;5;241m=\u001b[39m RBM(p_X, q)\n\u001b[0;32m---> 10\u001b[0m rbm\u001b[38;5;241m.\u001b[39mtrain_rbm(X, epochs_rbm, learning_rate, batch_size)\n\u001b[1;32m     11\u001b[0m rbm\u001b[38;5;241m.\u001b[39mgenerate_image_rbm(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m5\u001b[39m, nb_gibbs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m, in \u001b[0;36mRBM.train_rbm\u001b[0;34m(self, X, epochs, learning_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m v1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m1\u001b[39m, p_v_1)\n\u001b[1;32m     45\u001b[0m p_h_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentree_sortie_rbm(v1)\n\u001b[0;32m---> 47\u001b[0m grad_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(v0\u001b[38;5;241m.\u001b[39mT, p_h_0) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(v1\u001b[38;5;241m.\u001b[39mT, p_h_1)\n\u001b[1;32m     48\u001b[0m grad_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(v0 \u001b[38;5;241m-\u001b[39m v1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m grad_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(p_h_0 \u001b[38;5;241m-\u001b[39m p_h_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = lire_alpha_digit(digits, [2])\n",
    "n_X, p_X = X.shape\n",
    "q = 100  # number of hidden values\n",
    "epochs_rbm = 1000\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "nb_gibbs = 200\n",
    "\n",
    "rbm = RBM(p_X, q)\n",
    "rbm.train_rbm(X, epochs_rbm, learning_rate, batch_size)\n",
    "rbm.generate_image_rbm(20, 16, 5, nb_gibbs, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8c49d",
   "metadata": {},
   "source": [
    "# Variations données train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "batch_size = 38\n",
    "nb_gibbs = 300\n",
    "\n",
    "# Variation du nombre de caractères à apprendre\n",
    "characters_sets = [\n",
    "    [i for i in range(2, 3)],  # One character\n",
    "    [i for i in range(2, 6)],\n",
    "    [i for i in range(2, 10)],\n",
    "    [i for i in range(2, 20)], \n",
    "    [i for i in range(2, 36)]  # 35 characters\n",
    "]\n",
    "\n",
    "for characters_to_learn in characters_sets:\n",
    "    # Charger les données pour les caractères spécifiés\n",
    "    X = lire_alpha_digit(digits, characters_to_learn)\n",
    "    \n",
    "    rbm = RBM(p_X, 300)\n",
    "    rbm.train_rbm(X, epochs, learning_rate, batch_size)\n",
    "    rbm.generate_image_rbm(20, 16, 5, nb_gibbs, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38add285",
   "metadata": {},
   "source": [
    "# Variations nombre de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lire_alpha_digit(digits, [3])\n",
    "nb_neurons = [10, 50, 100, 300]\n",
    "for q in nb_neurons:\n",
    "    rbm = RBM(p_X, 100)\n",
    "    rbm.train_rbm(X, epochs, learning_rate, batch_size)\n",
    "    rbm.generate_image_rbm(20, 16, 5, nb_gibbs, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c332b17",
   "metadata": {},
   "source": [
    "# DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ee470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes: tuple = layer_sizes\n",
    "        self.dbn: [RBM] = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            rbm = RBM(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.dbn.append(rbm)\n",
    "\n",
    "    def train_dbn(self, X, epochs, learning_rate, batch_size):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            #print(\"Pre-training RBM %d...\" % i)\n",
    "            self.dbn[i] = self.dbn[i].train_rbm(\n",
    "                X=X,\n",
    "                epochs=epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            # Calcul des sorties de l'i-ème RBM pour les données d'entrée\n",
    "            X = np.random.binomial(1, self.dbn[i].entree_sortie_rbm(X))\n",
    "        return self\n",
    "\n",
    "    def generate_image_dbn(self, x_im, y_im, nb_data=1, nb_gibbs=100, plot=False):\n",
    "        v = self.dbn[-1].generate_image_rbm(x_im, y_im, nb_data, nb_gibbs)\n",
    "        for i in range(2, len(self.layer_sizes)):\n",
    "            v = np.random.binomial(1, self.dbn[-i].sortie_entree_rbm(v))\n",
    "\n",
    "        images = v.reshape((nb_data, x_im, y_im))\n",
    "        if plot:\n",
    "            # Reshape and Plot generated images\n",
    "            fig, axes = plt.subplots(1, nb_data, figsize=(10, 2))\n",
    "            for i in range(nb_data):\n",
    "                axes[i].imshow(images[i], cmap=\"gray\")\n",
    "                axes[i].axis(\"off\")\n",
    "            #plt.suptitle(f\"The config considered {self.layer_sizes}\")\n",
    "            plt.show()\n",
    "        #return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce252a",
   "metadata": {},
   "source": [
    "# Variations données train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed configuration\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "batch_size = 38\n",
    "nb_gibbs = 300\n",
    "\n",
    "# Variation du nombre de caractères à apprendre\n",
    "characters_sets = [\n",
    "    [i for i in range(2, 3)],# One character\n",
    "    [i for i in range(2, 6)],\n",
    "    [i for i in range(2, 10)],\n",
    "    [i for i in range(2, 20)], \n",
    "    [i for i in range(2, 36)]  # 35 characters\n",
    "    # Ajoutez plus d'ensembles au besoin\n",
    "]\n",
    "\n",
    "for characters_to_learn in characters_sets:\n",
    "    # Charger les données pour les caractères spécifiés\n",
    "    X = lire_alpha_digit(digits, characters_to_learn)\n",
    "    config = [p_X, 300]\n",
    "    dbn = DBN(config)\n",
    "    dbn.train_dbn(X, epochs, learning_rate, batch_size)\n",
    "    dbn.generate_image_dbn(20, 16, 4, nb_gibbs, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337c54b",
   "metadata": {},
   "source": [
    "# Variation nombre de couches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361ac87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = lire_alpha_digit(digits, [3])\n",
    "nb_layers = [1,5,10]\n",
    "for layer in nb_layers:\n",
    "    # Define the config for each iteration\n",
    "    config = [p_X]\n",
    "    config.extend([200 for x in range(layer)])\n",
    "\n",
    "    dbn = DBN(config)\n",
    "    dbn.train_dbn(X, 100, learning_rate, batch_size)\n",
    "    dbn.generate_image_dbn(20, 16, 4, nb_gibbs, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd14b2e",
   "metadata": {},
   "source": [
    "# Variation nombre de neurones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_neurons = [10,50,100]\n",
    "for neurons in nb_neurons:\n",
    "    # Define the config for each iteration\n",
    "    config = [p_X]\n",
    "    config.extend([neurons for x in range(2)])\n",
    "\n",
    "    dbn = DBN(config)\n",
    "    dbn.train_dbn(X, 100, learning_rate, batch_size)\n",
    "    dbn.generate_image_dbn(20, 16, 4, nb_gibbs, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d10b6a",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "def calcul_softmax(rbm: RBM, data):\n",
    "    z = np.array(rbm.b) + np.dot(data, rbm.W)\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, config, output_dim=10):\n",
    "\n",
    "        self.config: tuple = config\n",
    "        self.num_layers: int = len(self.config)  # number of layers except output\n",
    "\n",
    "        # un DNN est un DBN avec une couche de classification supplémentaire\n",
    "        # dernier RBM du DBN pour la classification → on ne définit pas \"rbm.a\".\n",
    "        self.dbn: DBN = DBN(config)\n",
    "        self.classification: RBM = RBM(config[-1], output_dim)\n",
    "        self.pretrained: bool = False  # check if model is pretrained\n",
    "        self.fitted: bool = False  # check if model is fitted\n",
    "\n",
    "    def pretrain_dnn(self, data, epochs=100, learning_rate=0.1, batch_size=100):\n",
    "        self.dbn.train_dbn(data, epochs, learning_rate, batch_size)\n",
    "        self.pretrained = True\n",
    "        return self\n",
    "\n",
    "    def entree_sortie_network(self, data):\n",
    "        v = data.copy()\n",
    "        results = [v]  # Couche d'entrée\n",
    "        for i in range(self.num_layers - 1):\n",
    "            p_h = self.dbn.dbn[i].entree_sortie_rbm(v)\n",
    "            v = np.random.binomial(1, p_h)\n",
    "            results.append(p_h)\n",
    "\n",
    "        # Compute the probabilities\n",
    "        softmax_probas = calcul_softmax(self.classification, v)\n",
    "        results.append(softmax_probas)\n",
    "        return results\n",
    "\n",
    "    def backward_propagation(self, data,labels,epochs=100,learning_rate=0.1,batch_size=100,early_stopping=5,verbose=True,plot=True,):\n",
    "        keep_track = 0\n",
    "        train_loss = 100\n",
    "        loss_batches, loss = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            data_copy = data.copy()\n",
    "            labels_copy = pd.get_dummies(labels.copy())\n",
    "            data_copy, labels_copy = shuffle(data_copy, labels_copy)\n",
    "\n",
    "            for batch in range(0, data.shape[0], batch_size):\n",
    "                data_batch = data_copy[\n",
    "                    batch: min(batch + batch_size, data.shape[0]), :\n",
    "                ]\n",
    "                labels_batch = labels_copy[\n",
    "                    batch: min(batch + batch_size, data.shape[0])\n",
    "                ]\n",
    "                # Forward pass\n",
    "                activations = self.entree_sortie_network(data_batch)\n",
    "\n",
    "                # Loss\n",
    "                loss_batches.append(\n",
    "                    -np.mean(np.sum(labels_batch * np.log(activations[-1]), axis=1))\n",
    "                )\n",
    "\n",
    "                # Backward pass\n",
    "                # Start with last layer\n",
    "                delta = activations[-1] - labels_batch\n",
    "                grad_w = np.dot(activations[-2].T, delta) / batch_size\n",
    "                grad_b = np.mean(delta, axis=0)\n",
    "                self.classification.W -= learning_rate * grad_w\n",
    "                self.classification.b -= learning_rate * grad_b\n",
    "\n",
    "                # Propagate error backwards through hidden layers\n",
    "                for layer in range(1, self.num_layers):\n",
    "                    if layer == 1:\n",
    "                        delta = np.dot(delta, self.classification.W.T) * sigmoid_prime(\n",
    "                            activations[-layer - 1]\n",
    "                        )\n",
    "                    else:\n",
    "                        delta = np.dot(\n",
    "                            delta, self.dbn.dbn[-layer + 1].W.T\n",
    "                        ) * sigmoid_prime(activations[-layer - 1])\n",
    "                    if layer == self.num_layers - 1:\n",
    "                        grad_w = np.dot(data_batch.T, delta) / batch_size\n",
    "                    else:\n",
    "                        grad_w = np.dot(activations[-layer - 2].T, delta) / batch_size\n",
    "                    grad_b = np.mean(delta, axis=0)\n",
    "                    self.dbn.dbn[-layer].W -= learning_rate * grad_w\n",
    "                    self.dbn.dbn[-layer].b -= learning_rate * grad_b\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            previous_loss = train_loss\n",
    "            train_loss = float(np.mean(loss_batches))\n",
    "            loss.append(train_loss)\n",
    "\n",
    "            if keep_track < early_stopping and round(train_loss, 3) == round(previous_loss, 3):\n",
    "                keep_track += 1\n",
    "            elif keep_track == early_stopping:\n",
    "                return self\n",
    "            # Print progress\n",
    "            #if verbose:\n",
    "             #   print(\n",
    "               #     f\"Epoch {epoch}/{epochs}: Train error -----------------{train_loss:.4f}\"\n",
    "              #  )\n",
    "        if plot:\n",
    "            plt.plot(np.arange(epochs), loss)\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"CrossEntropy Loss\")\n",
    "            if self.pretrained:\n",
    "                plt.title(\"Loss for pretrained DNN\")\n",
    "            else:\n",
    "                plt.title(\"Loss for DNN (without pretraining)\")\n",
    "            plt.show()\n",
    "\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def test_dnn(self, test_data, test_labels, verbose=True):\n",
    "        probs = self.entree_sortie_network(test_data)\n",
    "        pred_label = np.argmax(probs[-1], axis=1)\n",
    "        num_correct = np.sum(test_labels != pred_label)\n",
    "\n",
    "        # Print the error rate and return it\n",
    "        error_rate = num_correct / test_data.shape[0]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Error rate ----------------- : {error_rate:.2%}\")\n",
    "        return error_rate\n",
    "\n",
    "    def plot_proba(self, data):\n",
    "        pred_labels = self.entree_sortie_network(data)[-1]\n",
    "        plt.scatter(np.arange(0, 10), pred_labels[0])\n",
    "        plt.xlabel(\"Classes\")\n",
    "        plt.ylabel(\"Predicted probability for each class\")\n",
    "        plt.title(\"Probabilities by class\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(error_rate1, error_rate2, parameters, parameter_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "    # Plot of the error rate by parameter for pretrained network\n",
    "    axes[0].plot(parameters, error_rate1)\n",
    "    axes[0].set_title(\"Impact of number of layers on pretrained network\")\n",
    "    axes[0].set_xlabel(parameter_name)\n",
    "    axes[0].set_ylabel(\"Error rate\")\n",
    "\n",
    "    # Plot of the error rate by parameter for neural network\n",
    "    axes[1].plot(parameters, error_rate2)\n",
    "    axes[1].set_title(\"Impact of number of layers on neural network\")\n",
    "    axes[1].set_xlabel(parameter_name)\n",
    "    axes[1].set_ylabel(\"Error rate\")\n",
    "\n",
    "    plt.suptitle(f\"Comparing error rate of both models by {parameter_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(filename: str) -> DNN:\n",
    "    \"\"\"\n",
    "    :param filename: file path\n",
    "    :return: trained model\n",
    "    \"\"\"\n",
    "    with open(\"models/\"+filename, \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "        return model\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_model(filename: str, model):\n",
    "    \"\"\"\n",
    "    :param filename: file path\n",
    "    :param model: trained model to save\n",
    "    \"\"\"\n",
    "    directory = \"models/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open(os.path.join(directory, filename), \"wb\") as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d19c79",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lire_mnist(filename: str, indices: np.ndarray, data_type: str):\n",
    "    mnist_all = sp.io.loadmat(filename, simplify_cells=True)\n",
    "    key = data_type + \"0\"\n",
    "    data_mnist = (mnist_all[key] > 127).astype(int)\n",
    "    label = np.zeros(mnist_all[key].shape[0])\n",
    "    for i in indices[1:]:\n",
    "        key = data_type + str(i)\n",
    "        data_mnist = np.vstack([data_mnist, (mnist_all[key] > 127).astype(int)])\n",
    "        y = i * np.ones(mnist_all[key].shape[0])\n",
    "        label = np.concatenate([label, y], axis=0)\n",
    "    return data_mnist, label\n",
    "\n",
    "binaire = np.arange(0, 10)\n",
    "\n",
    "file = \"mnist_all.mat\"\n",
    "\n",
    "mnist_train, label_train = lire_mnist(file, binaire, \"train\")\n",
    "mnist_test, label_test = lire_mnist(file, binaire, \"test\")\n",
    "n_mnist, p_mnist = mnist_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08457d",
   "metadata": {},
   "source": [
    "# Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd38360",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_rbm = 100\n",
    "epochs_dnn = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "nb_gibbs = 200\n",
    "digits = np.arange(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6ce18",
   "metadata": {},
   "source": [
    "# TEST 1 : DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ff5ea",
   "metadata": {},
   "source": [
    "\n",
    "• Spécifier les paramètres liés au réseau et à l’apprentissage : taille du réseau (vecteur contenant le nombre de neurones), nombre d’itérations pour les descentes de gradient (100 pour les RBM, 200 pour l’algorithme de rétro-propagation du gradient), learning rate (ex : 0.1), taille des mini-batch, le nombre de données d’apprentissage, ...\n",
    "\n",
    "• Charger et binariser les données;\n",
    "\n",
    "• Initialisation aléatoire du DNN;\n",
    "\n",
    "• Si pré-apprentissage, pré-entraîner de manière non supervisée le DNN;\n",
    "\n",
    "• EntraînerdemanièresuperviséleDNNpréalablementpré-entrainévial’algorithmederétro-propagation du gradient.\n",
    "\n",
    "• Avec le réseau appris, observer les probabilités de sortie de quelques images de la base d’apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a146e",
   "metadata": {},
   "source": [
    "## Train (with pretraining) DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f39675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez les paramètres\n",
    "config = (p_mnist, 200, 200)\n",
    "train_model = True  # Change to False if you want to test instead of train\n",
    "\n",
    "# Entraînement du modèle\n",
    "if train_model:\n",
    "    dnn = DNN(config)\n",
    "    dnn.pretrain_dnn(mnist_train, epochs_rbm, learning_rate, batch_size)\n",
    "    dnn.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size,plot=False)\n",
    "\n",
    "    # Sauvegarde du modèle\n",
    "    path = f\"dnn_testPretrained{dnn.pretrained}.pkl\"\n",
    "    save_model(path, dnn)\n",
    "\n",
    "# Test du modèle\n",
    "else:\n",
    "    # Importez le modèle pré-entraîné\n",
    "    path = \"dnn_testPretrainedTrue.pkl\"  # Assurez-vous que le nom de fichier correspond au modèle pré-entraîné\n",
    "    dnn = import_model(path)\n",
    "\n",
    "# Test du modèle\n",
    "print(f\"Modèle DNN avec pré-entraînement={dnn.pretrained}:\")\n",
    "dnn.test_dnn(mnist_test, label_test)\n",
    "\n",
    "# Tracé des probabilités de la classe k\n",
    "k = 2  # Choisissez une classe et tracez la probabilité\n",
    "idx = np.where(label_test == k)[0]\n",
    "dnn.plot_proba(mnist_test[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedcf50",
   "metadata": {},
   "source": [
    "# TEST 2 DNN : pretrained VS not pretrained "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d165428",
   "metadata": {},
   "source": [
    "deuxieme test : copier la partie correspondante TP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9349a",
   "metadata": {},
   "source": [
    "1. initialiser deux réseaux identiques;\n",
    "2. pré-apprendre un des deux réseau en le considérant comme un empilement de RBM (apprentissage non supervisé);\n",
    "3. apprendre le réseau pré-appris préalablement avec l’algorithme de rétro-propagation;\n",
    "4. apprendre le second réseau qui a été initialisé aléatoirement avec l’algorithme de rétro-propagation;\n",
    "5. Calculer les taux de mauvaises classifications avec le réseau 1 (pré-entrainé + entraîné) et le réseau 2 (entraîné) à partir du jeu ’train’ et du jeu ’test’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa47c0",
   "metadata": {},
   "source": [
    "Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez les paramètres\n",
    "config = (p_mnist, 200, 200)\n",
    "train_models = True  # Change to False if you want to test instead of train\n",
    "\n",
    "# Test 2 DNN := pretrained VS not pretrained (5.2)\n",
    "if train_models:\n",
    "    # 1\n",
    "    dnn1 = DNN(config)  # to pretrain\n",
    "    dnn2 = DNN(config)\n",
    "\n",
    "    # 2\n",
    "    dnn1.pretrain_dnn(mnist_train, epochs_rbm, learning_rate, batch_size)\n",
    "\n",
    "    # 3\n",
    "    dnn1.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "    # 4\n",
    "    dnn2.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "    # Sauvegardez les deux modèles\n",
    "    path_1 = f\"dnn_testPretrained{dnn1.pretrained}.pkl\"\n",
    "    path_2 = f\"dnn_testPretrained{dnn2.pretrained}.pkl\"\n",
    "    save_model(path_1, dnn1)\n",
    "    save_model(path_2, dnn2)\n",
    "\n",
    "else:\n",
    "    # Importez les modèles\n",
    "    path_1 = \"dnn_testPretrainedTrue.pkl\"\n",
    "    dnn1 = import_model(path_1)\n",
    "    path_2 = \"dnn_testPretrainedFalse.pkl\"\n",
    "    dnn2 = import_model(path_2)\n",
    "\n",
    "# 5\n",
    "print(f\"Modèle DNN avec pré-entraînement={dnn1.pretrained} pour le jeu de données d'entraînement:\")\n",
    "dnn1.test_dnn(mnist_train, label_train)\n",
    "\n",
    "print(f\"Modèle DNN avec pré-entraînement={dnn1.pretrained} pour le jeu de données de test:\")\n",
    "dnn1.test_dnn(mnist_test, label_test)\n",
    "\n",
    "print(f\"Modèle DNN avec pré-entraînement={dnn2.pretrained} pour le jeu de données d'entraînement:\")\n",
    "dnn2.test_dnn(mnist_train, label_train)\n",
    "\n",
    "print(f\"Modèle DNN avec pré-entraînement={dnn2.pretrained} pour le jeu de données de test:\")\n",
    "dnn2.test_dnn(mnist_test, label_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a17f6",
   "metadata": {},
   "source": [
    "# ANALYSE : GRAPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab1c68",
   "metadata": {},
   "source": [
    "Test with different number layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fdaac",
   "metadata": {},
   "source": [
    "2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de couches (par exemple 2 couches de 200, puis 3 couches de 200, ... puis 5 couches de 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ff463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définissez les paramètres\n",
    "config = (p_mnist, 200, 200)\n",
    "train_models = True  # Change to False if you want to test instead of train\n",
    "test_nb_layers = True  # Change to False if you don't want to test with different number of layers\n",
    "\n",
    "# Test avec différents nombres de couches (5.2.2)\n",
    "if test_nb_layers:\n",
    "    error_rate1, error_rate2 = [], []\n",
    "    nb_layers = [2, 3, 4, 5]\n",
    "\n",
    "    for layer in nb_layers:\n",
    "        # Définissez la configuration pour chaque itération\n",
    "        config = [p_mnist]\n",
    "        config.extend([200 for _ in range(layer)])\n",
    "\n",
    "        if train_models:\n",
    "            dnn1 = DNN(config)  # pour pré-entraîner\n",
    "            dnn2 = DNN(config)\n",
    "\n",
    "            # Pré-entraîner le modèle puis l'entraîner\n",
    "            dnn1.pretrain_dnn(mnist_train, epochs_rbm, learning_rate, batch_size)\n",
    "            dnn1.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Entraîner le modèle directement\n",
    "            dnn2.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Sauvegardez les deux modèles\n",
    "            path_1 = f\"dnn_testPretrained{dnn1.pretrained}_NbLayers{str(layer)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrained{dnn2.pretrained}_NbLayers{str(layer)}.pkl\"\n",
    "            save_model(path_1, dnn1)\n",
    "            save_model(path_2, dnn2)\n",
    "\n",
    "        else:\n",
    "            # Importez les modèles\n",
    "            path_1 = f\"dnn_testPretrainedTrue_NbLayers{str(layer)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrainedFalse_NbLayers{str(layer)}.pkl\"\n",
    "            dnn1 = import_model(path_1)\n",
    "            dnn2 = import_model(path_2)\n",
    "\n",
    "        # Calculez le taux d'erreur pour les deux modèles\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn1.pretrained} pour nb_layer={layer}:\")\n",
    "        error1 = dnn1.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn2.pretrained} pour nb_layer={layer}:\")\n",
    "        error2 = dnn2.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        error_rate1.append(error1)\n",
    "        error_rate2.append(error2)\n",
    "\n",
    "    # Tracer le taux d'erreur en fonction du nombre de couches\n",
    "    plot_loss(error_rate1, error_rate2, nb_layers, \"Nombre de couches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e43314",
   "metadata": {},
   "source": [
    "Test with different number neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e77107c",
   "metadata": {},
   "source": [
    "2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de neurones par couches (par exemple 2 couches de 100, puis 2 couches de 300, ...puis 2 couches de 700,...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13798f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Définissez les paramètres\n",
    "config = (p_mnist, 200, 200)\n",
    "train_models = True  # Change to False if you want to test instead of train\n",
    "test_nb_neurons = True  # Change to False if you don't want to test with different number of neurons\n",
    "\n",
    "# Test avec différents nombres de neurones (5.2.2)\n",
    "if test_nb_neurons:\n",
    "    error_rate1, error_rate2 = [], []\n",
    "    nb_neurons = [100, 200, 300, 700]\n",
    "\n",
    "    for neurons in nb_neurons:\n",
    "        # Définissez la configuration pour chaque itération\n",
    "        config = [p_mnist]\n",
    "        config.extend([neurons for _ in range(2)])\n",
    "\n",
    "        if train_models:\n",
    "            dnn1 = DNN(config)  # pour pré-entraîner\n",
    "            dnn2 = DNN(config)\n",
    "\n",
    "            # Pré-entraîner le modèle puis l'entraîner\n",
    "            dnn1.pretrain_dnn(mnist_train, epochs_rbm, learning_rate, batch_size)\n",
    "            dnn1.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Entraîner le modèle directement\n",
    "            dnn2.backward_propagation(mnist_train, label_train, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Sauvegardez les deux modèles\n",
    "            path_1 = f\"dnn_testPretrained{dnn1.pretrained}_NbNeurons{str(neurons)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrained{dnn2.pretrained}_NbNeurons{str(neurons)}.pkl\"\n",
    "            save_model(path_1, dnn1)\n",
    "            save_model(path_2, dnn2)\n",
    "\n",
    "        else:\n",
    "            # Importez les modèles\n",
    "            path_1 = f\"dnn_testPretrainedTrue_NbNeurons{str(neurons)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrainedFalse_NbNeurons{str(neurons)}.pkl\"\n",
    "            dnn1 = import_model(path_1)\n",
    "            dnn2 = import_model(path_2)\n",
    "\n",
    "        # Calculez le taux d'erreur pour les deux modèles\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn1.pretrained} pour nb_neurons={neurons}:\")\n",
    "        error1 = dnn1.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn2.pretrained} pour nb_neurons={neurons}:\")\n",
    "        error2 = dnn2.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        error_rate1.append(error1)\n",
    "        error_rate2.append(error2)\n",
    "\n",
    "    # Tracer le taux d'erreur en fonction du nombre de neurones\n",
    "    plot_loss(error_rate1, error_rate2, nb_neurons, \"Nombre de neurones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893e8fd",
   "metadata": {},
   "source": [
    "Test with different size of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25960d8",
   "metadata": {},
   "source": [
    "2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de données train (par exemple on fixe 2 couches de 200 puis on utilise 1000 données train, 3000, 7000, 10000, 30000, 60000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40672bcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Définissez les paramètres\n",
    "config = (p_mnist, 200, 200)\n",
    "train_models = True  # Change to False if you want to test instead of train\n",
    "test_train_size = True  # Change to False if you don't want to test with different sizes of training data\n",
    "\n",
    "# Test avec différentes tailles de données d'entraînement (5.2.2)\n",
    "if test_train_size:\n",
    "    error_rate1, error_rate2 = [], []\n",
    "    train_sizes = [1000, 3000, 7000, 10000, 30000, 60000]\n",
    "\n",
    "    for size in train_sizes:\n",
    "        config = (p_mnist, 200, 200)\n",
    "\n",
    "        if train_models:\n",
    "            dnn1 = DNN(config)  # pour pré-entraîner\n",
    "            dnn2 = DNN(config)\n",
    "\n",
    "            # Sélectionnez les données d'entraînement pour chaque itération\n",
    "            data_shuffled, labels_shuffled = shuffle(mnist_train, label_train)\n",
    "            data_sampled, labels_sampled = data_shuffled[:size], labels_shuffled[:size]\n",
    "\n",
    "            # Pré-entraîner le modèle puis l'entraîner\n",
    "            dnn1.pretrain_dnn(data_sampled, epochs_rbm, learning_rate, batch_size)\n",
    "            dnn1.backward_propagation(data_sampled, labels_sampled, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Entraîner le modèle directement\n",
    "            dnn2.backward_propagation(data_sampled, labels_sampled, epochs_dnn, learning_rate, batch_size, plot=False)\n",
    "\n",
    "            # Sauvegardez les deux modèles\n",
    "            path_1 = f\"dnn_testPretrained{dnn1.pretrained}_TrainSize{str(size)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrained{dnn2.pretrained}_TrainSize{str(size)}.pkl\"\n",
    "            save_model(path_1, dnn1)\n",
    "            save_model(path_2, dnn2)\n",
    "\n",
    "        else:\n",
    "            # Importez les modèles\n",
    "            path_1 = f\"dnn_testPretrainedTrue_TrainSize{str(size)}.pkl\"\n",
    "            path_2 = f\"dnn_testPretrainedFalse_TrainSize{str(size)}.pkl\"\n",
    "            dnn1 = import_model(path_1)\n",
    "            dnn2 = import_model(path_2)\n",
    "\n",
    "        # Calculez le taux d'erreur pour les deux modèles\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn1.pretrained} pour train_size={size}:\")\n",
    "        error1 = dnn1.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        print(f\"Modèle DNN avec pré-entraînement={dnn2.pretrained} pour train_size={size}:\")\n",
    "        error2 = dnn2.test_dnn(mnist_test, label_test)\n",
    "\n",
    "        error_rate1.append(error1)\n",
    "        error_rate2.append(error2)\n",
    "\n",
    "    # Tracer le taux d'erreur en fonction de la taille de l'échantillon\n",
    "    plot_loss(error_rate1, error_rate2, train_sizes, \"Taille de l'échantillon\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
